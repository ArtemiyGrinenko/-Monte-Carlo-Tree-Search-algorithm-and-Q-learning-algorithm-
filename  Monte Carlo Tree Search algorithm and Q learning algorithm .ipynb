{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2bafa838-1dc4-4f87-a086-9d8f392a95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train the Monte Carlo Tree Search agent  and Q-learning agents do it in two steps :\n",
    "\n",
    "# Train MCTS using random moves: This will allow the MCTS agent to explore and learn by playing against random moves, which should give it a chance to discover various strategies.\n",
    "# Train Q learning with MCTS and random moves: The Q learning agent will train against the MCTS agent (with a mix of random moves), allowing it to explore various strategies.\n",
    "# adjust the epsilon decay in the Q learning agent to ensure that it explores adequately in the beginning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4dc1682-855e-4c8e-96e1-ee739961ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Training MCTS with Random Moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3bca9c5-335d-4d29-8df7-c5cac9878c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 games (MCTS vs Random):\n",
      "Random Agent Wins: 54\n",
      "MCTS Agent Wins: 46\n",
      "Draws: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Connect Four Environment\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect Four Environment\n",
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.rows = 6\n",
    "        self.cols = 7\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return tuple(map(tuple, self.board))\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if not self.is_valid_move(col):\n",
    "            return False, None\n",
    "        for row in range(self.rows - 1, -1, -1):\n",
    "            if self.board[row, col] == 0:\n",
    "                self.board[row, col] = self.current_player\n",
    "                reward, done = self.check_game_status()\n",
    "                self.current_player = 3 - self.current_player\n",
    "                return reward, done\n",
    "        return 0, False\n",
    "\n",
    "    def check_game_status(self):\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.cols - 3):\n",
    "                if self.check_sequence(row, col, 0, 1):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols):\n",
    "                if self.check_sequence(row, col, 1, 0):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols - 3):\n",
    "                if self.check_sequence(row, col, 1, 1) or self.check_sequence(row + 3, col, -1, 1):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        if not any(self.is_valid_move(c) for c in range(self.cols)):\n",
    "            return (0, True)  # Draw case\n",
    "        return (0, False)\n",
    "\n",
    "    def check_sequence(self, row, col, row_delta, col_delta):\n",
    "        piece = self.board[row, col]\n",
    "        if piece == 0:\n",
    "            return False\n",
    "        for i in range(1, 4):\n",
    "            if self.board[row + i * row_delta, col + i * col_delta] != piece:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "# Q learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.3, gamma=0.95, epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.05):\n",
    "        self.q_table = defaultdict(lambda: np.zeros(7))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def choose_action(self, state, valid_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(valid_moves)\n",
    "        q_values = self.q_table[state]\n",
    "        return max(valid_moves, key=lambda col: q_values[col])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        q_values = self.q_table[state]\n",
    "        if done:\n",
    "            q_values[action] += self.alpha * (reward - q_values[action])\n",
    "        else:\n",
    "            next_q_values = self.q_table[next_state]\n",
    "            q_values[action] += self.alpha * (reward + self.gamma * np.max(next_q_values) - q_values[action])\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "# MCTS Node\n",
    "class Node:\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "        self.children = {}\n",
    "\n",
    "    def best_child(self):\n",
    "        return max(self.children.items(), key=lambda x: x[1].wins / (x[1].visits + 1e-6))[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class MCTS:\n",
    "    def __init__(self, simulations=50):\n",
    "        self.simulations = simulations\n",
    "\n",
    "    def search(self, env, root):\n",
    "        for _ in range(self.simulations):\n",
    "            node = root\n",
    "            temp_env = ConnectFour()\n",
    "            temp_env.board = np.copy(env.board)\n",
    "            temp_env.current_player = env.current_player\n",
    "\n",
    "            # Simulate the game from the current node state\n",
    "            while node.children:\n",
    "                action = node.best_child()\n",
    "                temp_env.make_move(action)\n",
    "                node = node.children[action]\n",
    "\n",
    "            # Add children nodes for all valid moves\n",
    "            valid_moves = [c for c in range(env.cols) if temp_env.is_valid_move(c)]\n",
    "            for move in valid_moves:\n",
    "                temp_env.make_move(move)\n",
    "                node.children[move] = Node(temp_env.get_state())\n",
    "\n",
    "            # Simulate outcome\n",
    "            reward, done = temp_env.check_game_status()\n",
    "            if done:\n",
    "                for child in node.children.values():\n",
    "                    child.wins += reward\n",
    "                    child.visits += 1\n",
    "\n",
    "        return root.best_child()\n",
    "\n",
    "def train_mcts_random(env, mcts_agent, games=100):\n",
    "    random_wins, mcts_wins, draws = 0, 0, 0\n",
    "\n",
    "    for game in range(games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "            action = random.choice(valid_moves)  # Random agent\n",
    "            reward, done = env.make_move(action)\n",
    "            state = env.get_state()\n",
    "\n",
    "        if reward == 1:\n",
    "            random_wins += 1\n",
    "        elif reward == -1:\n",
    "            mcts_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "        #print(f\"Game {game + 1}/{games} completed.\")\n",
    "\n",
    "    print(f\"Results after {games} games (MCTS vs Random):\\nRandom Agent Wins: {random_wins}\\nMCTS Agent Wins: {mcts_wins}\\nDraws: {draws}\")\n",
    "\n",
    "# Train MCTS agent against random moves\n",
    "train_mcts_random(ConnectFour(), MCTS(simulations=40), games=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d202185-5ccf-4c35-8168-9be7b048794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Training Q-learning with MCTS and Random Moves\n",
    "# Now,  train the Q-learning agent by having it play against the MCTS agent and a random agent.\n",
    "# use a mix of epsilon-greedy exploration for the Q-learning agent and ensure that the agent explores sufficiently early on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df2c9ccc-fb3a-42c4-90a7-48eb8bc8a79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500/10000 - Epsilon: 0.0500\n",
      "Episode 1000/10000 - Epsilon: 0.0500\n",
      "Episode 1500/10000 - Epsilon: 0.0500\n",
      "Episode 2000/10000 - Epsilon: 0.0500\n",
      "Episode 2500/10000 - Epsilon: 0.0500\n",
      "Episode 3000/10000 - Epsilon: 0.0500\n",
      "Episode 3500/10000 - Epsilon: 0.0500\n",
      "Episode 4000/10000 - Epsilon: 0.0500\n",
      "Episode 4500/10000 - Epsilon: 0.0500\n",
      "Episode 5000/10000 - Epsilon: 0.0500\n",
      "Episode 5500/10000 - Epsilon: 0.0500\n",
      "Episode 6000/10000 - Epsilon: 0.0500\n",
      "Episode 6500/10000 - Epsilon: 0.0500\n",
      "Episode 7000/10000 - Epsilon: 0.0500\n",
      "Episode 7500/10000 - Epsilon: 0.0500\n",
      "Episode 8000/10000 - Epsilon: 0.0500\n",
      "Episode 8500/10000 - Epsilon: 0.0500\n",
      "Episode 9000/10000 - Epsilon: 0.0500\n",
      "Episode 9500/10000 - Epsilon: 0.0500\n",
      "Episode 10000/10000 - Epsilon: 0.0500\n",
      "Game 1/100 completed.\n",
      "Game 2/100 completed.\n",
      "Game 3/100 completed.\n",
      "Game 4/100 completed.\n",
      "Game 5/100 completed.\n",
      "Game 6/100 completed.\n",
      "Game 7/100 completed.\n",
      "Game 8/100 completed.\n",
      "Game 9/100 completed.\n",
      "Game 10/100 completed.\n",
      "Game 11/100 completed.\n",
      "Game 12/100 completed.\n",
      "Game 13/100 completed.\n",
      "Game 14/100 completed.\n",
      "Game 15/100 completed.\n",
      "Game 16/100 completed.\n",
      "Game 17/100 completed.\n",
      "Game 18/100 completed.\n",
      "Game 19/100 completed.\n",
      "Game 20/100 completed.\n",
      "Game 21/100 completed.\n",
      "Game 22/100 completed.\n",
      "Game 23/100 completed.\n",
      "Game 24/100 completed.\n",
      "Game 25/100 completed.\n",
      "Game 26/100 completed.\n",
      "Game 27/100 completed.\n",
      "Game 28/100 completed.\n",
      "Game 29/100 completed.\n",
      "Game 30/100 completed.\n",
      "Game 31/100 completed.\n",
      "Game 32/100 completed.\n",
      "Game 33/100 completed.\n",
      "Game 34/100 completed.\n",
      "Game 35/100 completed.\n",
      "Game 36/100 completed.\n",
      "Game 37/100 completed.\n",
      "Game 38/100 completed.\n",
      "Game 39/100 completed.\n",
      "Game 40/100 completed.\n",
      "Game 41/100 completed.\n",
      "Game 42/100 completed.\n",
      "Game 43/100 completed.\n",
      "Game 44/100 completed.\n",
      "Game 45/100 completed.\n",
      "Game 46/100 completed.\n",
      "Game 47/100 completed.\n",
      "Game 48/100 completed.\n",
      "Game 49/100 completed.\n",
      "Game 50/100 completed.\n",
      "Game 51/100 completed.\n",
      "Game 52/100 completed.\n",
      "Game 53/100 completed.\n",
      "Game 54/100 completed.\n",
      "Game 55/100 completed.\n",
      "Game 56/100 completed.\n",
      "Game 57/100 completed.\n",
      "Game 58/100 completed.\n",
      "Game 59/100 completed.\n",
      "Game 60/100 completed.\n",
      "Game 61/100 completed.\n",
      "Game 62/100 completed.\n",
      "Game 63/100 completed.\n",
      "Game 64/100 completed.\n",
      "Game 65/100 completed.\n",
      "Game 66/100 completed.\n",
      "Game 67/100 completed.\n",
      "Game 68/100 completed.\n",
      "Game 69/100 completed.\n",
      "Game 70/100 completed.\n",
      "Game 71/100 completed.\n",
      "Game 72/100 completed.\n",
      "Game 73/100 completed.\n",
      "Game 74/100 completed.\n",
      "Game 75/100 completed.\n",
      "Game 76/100 completed.\n",
      "Game 77/100 completed.\n",
      "Game 78/100 completed.\n",
      "Game 79/100 completed.\n",
      "Game 80/100 completed.\n",
      "Game 81/100 completed.\n",
      "Game 82/100 completed.\n",
      "Game 83/100 completed.\n",
      "Game 84/100 completed.\n",
      "Game 85/100 completed.\n",
      "Game 86/100 completed.\n",
      "Game 87/100 completed.\n",
      "Game 88/100 completed.\n",
      "Game 89/100 completed.\n",
      "Game 90/100 completed.\n",
      "Game 91/100 completed.\n",
      "Game 92/100 completed.\n",
      "Game 93/100 completed.\n",
      "Game 94/100 completed.\n",
      "Game 95/100 completed.\n",
      "Game 96/100 completed.\n",
      "Game 97/100 completed.\n",
      "Game 98/100 completed.\n",
      "Game 99/100 completed.\n",
      "Game 100/100 completed.\n",
      "Results after 100 games:\n",
      "Q-learning Agent Wins: 78\n",
      "MCTS Agent Wins: 13\n",
      "Draws: 9\n",
      "--- 526.0265848636627 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Q-learning Agent (same as before with epsilon decay adjustment)\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.3, gamma=0.95, epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.05):\n",
    "        self.q_table = defaultdict(lambda: np.zeros(7))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def choose_action(self, state, valid_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(valid_moves)\n",
    "        q_values = self.q_table[state]\n",
    "        return max(valid_moves, key=lambda col: q_values[col])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        q_values = self.q_table[state]\n",
    "        if done:\n",
    "            q_values[action] += self.alpha * (reward - q_values[action])\n",
    "        else:\n",
    "            next_q_values = self.q_table[next_state]\n",
    "            q_values[action] += self.alpha * (reward + self.gamma * np.max(next_q_values) - q_values[action])\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "# Training function with Q-learning playing against MCTS and random moves\n",
    "def train_q_with_mcts_and_random(q_agent, mcts_agent, episodes=50000):\n",
    "    env = ConnectFour()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "            \n",
    "            # Choose between Q learning, MCTS , or random\n",
    "            if random.uniform(0, 1) < 0.33:\n",
    "                action = q_agent.choose_action(state, valid_moves)  # Q-learning chooses move\n",
    "            elif random.uniform(0, 1) < 0.66:\n",
    "                root = Node(state)\n",
    "                action = mcts_agent.search(env, root)  # MCTS chooses move\n",
    "            else:\n",
    "                action = random.choice(valid_moves)  # Random move\n",
    "\n",
    "            reward, done = env.make_move(action)\n",
    "            next_state = env.get_state()\n",
    "            q_agent.update_q_table(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "        # Adjust exploration exploitation balance during training\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes} - Epsilon: {q_agent.epsilon:.4f}\")\n",
    "\n",
    "# Test the Q learning agent against MCTS and random agents after training\n",
    "def test_q_agent(q_agent, mcts_agent, games=100):\n",
    "    env = ConnectFour()\n",
    "    q_wins, mcts_wins, draws = 0, 0, 0\n",
    "    \n",
    "    for game in range(games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "            action = q_agent.choose_action(state, valid_moves) if random.uniform(0, 1) < 0.5 else mcts_agent.search(env, Node(state))\n",
    "            reward, done = env.make_move(action)\n",
    "            state = env.get_state()\n",
    "        \n",
    "        if reward == 1:\n",
    "            q_wins += 1\n",
    "        elif reward == -1:\n",
    "            mcts_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "        \n",
    "        print(f\"Game {game + 1}/{games} completed.\")\n",
    "    \n",
    "    print(f\"Results after {games} games:\\nQ-learning Agent Wins: {q_wins}\\nMCTS Agent Wins: {mcts_wins}\\nDraws: {draws}\")\n",
    "start_time = time.time()\n",
    "# Train Q learning with MCTS and random agent , then test the agent\n",
    "q_agent = QLearningAgent()\n",
    "mcts_agent = MCTS(simulations=20)\n",
    "train_q_with_mcts_and_random(q_agent, mcts_agent, episodes=10000)\n",
    "test_q_agent(q_agent, mcts_agent, games=100)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8960f395-db9e-443a-99aa-494f1cd372aa",
   "metadata": {},
   "outputs": [],
   "source": [
    " # both Q learning and MCTS are learning from each other while also training with random moves. The goal here is to train both algorithms simultaneously by allowing them to interact and adjust based on each other's actions , including random moves.\n",
    "\n",
    "#Key changes:\n",
    "# Q learning updates should occur based on the move chosen by MCTS or random moves.\n",
    "# MCTS updates should occur based on the reward from the environment after each move, similar to how Q learning works.\n",
    "# Random moves will also be involved in training by either the Q-learning or MCTS agent, simulating a diverse range of actions and scenarios.\n",
    "# Here is a complete rewrite that incorporates these updates:\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94cfc0a8-ae93-4ac8-bf8a-766efffc278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500/50000 - Epsilon: 0.1358\n",
      "Episode 1000/50000 - Epsilon: 0.0500\n",
      "Episode 1500/50000 - Epsilon: 0.0500\n",
      "Episode 2000/50000 - Epsilon: 0.0500\n",
      "Episode 2500/50000 - Epsilon: 0.0500\n",
      "Episode 3000/50000 - Epsilon: 0.0500\n",
      "Episode 3500/50000 - Epsilon: 0.0500\n",
      "Episode 4000/50000 - Epsilon: 0.0500\n",
      "Episode 4500/50000 - Epsilon: 0.0500\n",
      "Episode 5000/50000 - Epsilon: 0.0500\n",
      "Episode 5500/50000 - Epsilon: 0.0500\n",
      "Episode 6000/50000 - Epsilon: 0.0500\n",
      "Episode 6500/50000 - Epsilon: 0.0500\n",
      "Episode 7000/50000 - Epsilon: 0.0500\n",
      "Episode 7500/50000 - Epsilon: 0.0500\n",
      "Episode 8000/50000 - Epsilon: 0.0500\n",
      "Episode 8500/50000 - Epsilon: 0.0500\n",
      "Episode 9000/50000 - Epsilon: 0.0500\n",
      "Episode 9500/50000 - Epsilon: 0.0500\n",
      "Episode 10000/50000 - Epsilon: 0.0500\n",
      "Episode 10500/50000 - Epsilon: 0.0500\n",
      "Episode 11000/50000 - Epsilon: 0.0500\n",
      "Episode 11500/50000 - Epsilon: 0.0500\n",
      "Episode 12000/50000 - Epsilon: 0.0500\n",
      "Episode 12500/50000 - Epsilon: 0.0500\n",
      "Episode 13000/50000 - Epsilon: 0.0500\n",
      "Episode 13500/50000 - Epsilon: 0.0500\n",
      "Episode 14000/50000 - Epsilon: 0.0500\n",
      "Episode 14500/50000 - Epsilon: 0.0500\n",
      "Episode 15000/50000 - Epsilon: 0.0500\n",
      "Episode 15500/50000 - Epsilon: 0.0500\n",
      "Episode 16000/50000 - Epsilon: 0.0500\n",
      "Episode 16500/50000 - Epsilon: 0.0500\n",
      "Episode 17000/50000 - Epsilon: 0.0500\n",
      "Episode 17500/50000 - Epsilon: 0.0500\n",
      "Episode 18000/50000 - Epsilon: 0.0500\n",
      "Episode 18500/50000 - Epsilon: 0.0500\n",
      "Episode 19000/50000 - Epsilon: 0.0500\n",
      "Episode 19500/50000 - Epsilon: 0.0500\n",
      "Episode 20000/50000 - Epsilon: 0.0500\n",
      "Episode 20500/50000 - Epsilon: 0.0500\n",
      "Episode 21000/50000 - Epsilon: 0.0500\n",
      "Episode 21500/50000 - Epsilon: 0.0500\n",
      "Episode 22000/50000 - Epsilon: 0.0500\n",
      "Episode 22500/50000 - Epsilon: 0.0500\n",
      "Episode 23000/50000 - Epsilon: 0.0500\n",
      "Episode 23500/50000 - Epsilon: 0.0500\n",
      "Episode 24000/50000 - Epsilon: 0.0500\n",
      "Episode 24500/50000 - Epsilon: 0.0500\n",
      "Episode 25000/50000 - Epsilon: 0.0500\n",
      "Episode 25500/50000 - Epsilon: 0.0500\n",
      "Episode 26000/50000 - Epsilon: 0.0500\n",
      "Episode 26500/50000 - Epsilon: 0.0500\n",
      "Episode 27000/50000 - Epsilon: 0.0500\n",
      "Episode 27500/50000 - Epsilon: 0.0500\n",
      "Episode 28000/50000 - Epsilon: 0.0500\n",
      "Episode 28500/50000 - Epsilon: 0.0500\n",
      "Episode 29000/50000 - Epsilon: 0.0500\n",
      "Episode 29500/50000 - Epsilon: 0.0500\n",
      "Episode 30000/50000 - Epsilon: 0.0500\n",
      "Episode 30500/50000 - Epsilon: 0.0500\n",
      "Episode 31000/50000 - Epsilon: 0.0500\n",
      "Episode 31500/50000 - Epsilon: 0.0500\n",
      "Episode 32000/50000 - Epsilon: 0.0500\n",
      "Episode 32500/50000 - Epsilon: 0.0500\n",
      "Episode 33000/50000 - Epsilon: 0.0500\n",
      "Episode 33500/50000 - Epsilon: 0.0500\n",
      "Episode 34000/50000 - Epsilon: 0.0500\n",
      "Episode 34500/50000 - Epsilon: 0.0500\n",
      "Episode 35000/50000 - Epsilon: 0.0500\n",
      "Episode 35500/50000 - Epsilon: 0.0500\n",
      "Episode 36000/50000 - Epsilon: 0.0500\n",
      "Episode 36500/50000 - Epsilon: 0.0500\n",
      "Episode 37000/50000 - Epsilon: 0.0500\n",
      "Episode 37500/50000 - Epsilon: 0.0500\n",
      "Episode 38000/50000 - Epsilon: 0.0500\n",
      "Episode 38500/50000 - Epsilon: 0.0500\n",
      "Episode 39000/50000 - Epsilon: 0.0500\n",
      "Episode 39500/50000 - Epsilon: 0.0500\n",
      "Episode 40000/50000 - Epsilon: 0.0500\n",
      "Episode 40500/50000 - Epsilon: 0.0500\n",
      "Episode 41000/50000 - Epsilon: 0.0500\n",
      "Episode 41500/50000 - Epsilon: 0.0500\n",
      "Episode 42000/50000 - Epsilon: 0.0500\n",
      "Episode 42500/50000 - Epsilon: 0.0500\n",
      "Episode 43000/50000 - Epsilon: 0.0500\n",
      "Episode 43500/50000 - Epsilon: 0.0500\n",
      "Episode 44000/50000 - Epsilon: 0.0500\n",
      "Episode 44500/50000 - Epsilon: 0.0500\n",
      "Episode 45000/50000 - Epsilon: 0.0500\n",
      "Episode 45500/50000 - Epsilon: 0.0500\n",
      "Episode 46000/50000 - Epsilon: 0.0500\n",
      "Episode 46500/50000 - Epsilon: 0.0500\n",
      "Episode 47000/50000 - Epsilon: 0.0500\n",
      "Episode 47500/50000 - Epsilon: 0.0500\n",
      "Episode 48000/50000 - Epsilon: 0.0500\n",
      "Episode 48500/50000 - Epsilon: 0.0500\n",
      "Episode 49000/50000 - Epsilon: 0.0500\n",
      "Episode 49500/50000 - Epsilon: 0.0500\n",
      "Episode 50000/50000 - Epsilon: 0.0500\n",
      "--- 2800.8661670684814 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.rows = 6\n",
    "        self.cols = 7\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return tuple(map(tuple, self.board))\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if not self.is_valid_move(col):\n",
    "            return False, None\n",
    "        for row in range(self.rows - 1, -1, -1):\n",
    "            if self.board[row, col] == 0:\n",
    "                self.board[row, col] = self.current_player\n",
    "                reward, done = self.check_game_status()\n",
    "                self.current_player = 3 - self.current_player\n",
    "                return reward, done\n",
    "        return 0, False\n",
    "\n",
    "    def check_game_status(self):\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.cols - 3):\n",
    "                if self.check_sequence(row, col, 0, 1):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols):\n",
    "                if self.check_sequence(row, col, 1, 0):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols - 3):\n",
    "                if self.check_sequence(row, col, 1, 1) or self.check_sequence(row + 3, col, -1, 1):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        if not any(self.is_valid_move(c) for c in range(self.cols)):\n",
    "            return (0, True)  # Draw case\n",
    "        return (0, False)\n",
    "\n",
    "    def check_sequence(self, row, col, row_delta, col_delta):\n",
    "        piece = self.board[row, col]\n",
    "        if piece == 0:\n",
    "            return False\n",
    "        for i in range(1, 4):\n",
    "            if self.board[row + i * row_delta, col + i * col_delta] != piece:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "# Q learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.3, gamma=0.95, epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.05):\n",
    "        self.q_table = defaultdict(lambda: np.zeros(7))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def choose_action(self, state, valid_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(valid_moves)\n",
    "        q_values = self.q_table[state]\n",
    "        return max(valid_moves, key=lambda col: q_values[col])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        q_values = self.q_table[state]\n",
    "        if done:\n",
    "            q_values[action] += self.alpha * (reward - q_values[action])\n",
    "        else:\n",
    "            next_q_values = self.q_table[next_state]\n",
    "            q_values[action] += self.alpha * (reward + self.gamma * np.max(next_q_values) - q_values[action])\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "\n",
    "# MCTS Node\n",
    "class Node:\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "        self.children = {}\n",
    "\n",
    "    def best_child(self):\n",
    "        return max(self.children.items(), key=lambda x: x[1].wins / (x[1].visits + 1e-6))[0]\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, simulations=20, q_agent=None): \n",
    "        self.simulations = simulations\n",
    "        self.q_agent = q_agent  # Allow MCTS to use Q-learning agent\n",
    "\n",
    "    def search(self, env, root):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.run_simulation, env, root) for _ in range(self.simulations)]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "        return root.best_child()\n",
    "\n",
    "    def run_simulation(self, env, root):\n",
    "        node = root\n",
    "        temp_env = ConnectFour()\n",
    "        temp_env.board = np.copy(env.board)\n",
    "        temp_env.current_player = env.current_player\n",
    "\n",
    "        # Selection\n",
    "        while node.children:\n",
    "            action = node.best_child()\n",
    "            temp_env.make_move(action)\n",
    "            node = node.children[action]\n",
    "\n",
    "        # Expansion\n",
    "        valid_moves = [c for c in range(env.cols) if temp_env.is_valid_move(c)]\n",
    "        for move in valid_moves:\n",
    "            temp_env.make_move(move)\n",
    "            node.children[move] = Node(temp_env.get_state())\n",
    "\n",
    "        # Simulation: Random or Q learning move\n",
    "        state = temp_env.get_state()\n",
    "        done = False\n",
    "        reward = 0  # Initialize reward to ensure it's always assigned \n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if temp_env.is_valid_move(c)]\n",
    "            if not valid_moves:  # Handle case where no moves are left\n",
    "                break\n",
    "\n",
    "            if random.uniform(0, 1) < 0.5 and self.q_agent:\n",
    "                action = self.q_agent.choose_action(state, valid_moves)\n",
    "            else:\n",
    "                action = random.choice(valid_moves)\n",
    "            reward, done = temp_env.make_move(action) \n",
    "            state = temp_env.get_state()\n",
    "        \n",
    "        # Backpropagation\n",
    "        for child in node.children.values():\n",
    "            child.wins += reward\n",
    "            child.visits += 1\n",
    "\n",
    "\n",
    "# Simultaneous Training of Q learning and MCTS with Random Moves\n",
    "def train_both_agents(env, q_agent, mcts_agent, episodes=50000):\n",
    "\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "            \n",
    "            # Alternate between Q learning , MCTS, or Random Move\n",
    "            move_choice = random.uniform(0, 1)\n",
    "            if move_choice < 0.33:\n",
    "                action = q_agent.choose_action(state, valid_moves)  # Q learning move\n",
    "            elif move_choice < 0.66:\n",
    "                root = Node(state)\n",
    "                action = mcts_agent.search(env, root)  # MCTS move\n",
    "            else:\n",
    "                action = random.choice(valid_moves)  # Random move\n",
    "\n",
    "            reward, done = env.make_move(action)\n",
    "            next_state = env.get_state()\n",
    "\n",
    "            if move_choice < 0.33:  # Only update Q learning if it chose the move\n",
    "                q_agent.update_q_table(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Print progress every 500 episodes\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes} - Epsilon: {q_agent.epsilon:.4f}\")\n",
    "\n",
    "\n",
    "# Testing the trained Q-learning agent vs. MCTS agent\n",
    "def test_agents(q_agent, mcts_agent, games=100):\n",
    "    env = ConnectFour()\n",
    "    q_wins, mcts_wins, draws = 0, 0, 0\n",
    "    \n",
    "    for game in range(games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "            \n",
    "            # Alternate between Q-learning and MCTS during the game\n",
    "            move_choice = random.uniform(0, 1)\n",
    "            if move_choice < 0.5:\n",
    "                action = q_agent.choose_action(state, valid_moves)\n",
    "            else:\n",
    "                root = Node(state)\n",
    "                action = mcts_agent.search(env, root)\n",
    "            \n",
    "            reward, done = env.make_move(action)\n",
    "            state = env.get_state()\n",
    "        \n",
    "        if reward == 1:\n",
    "            q_wins += 1\n",
    "        elif reward == -1:\n",
    "            mcts_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "        \n",
    "        #print(f\"Game {game + 1}/{games} completed.\")\n",
    "    \n",
    "    print(f\"Results after {games} games:\\nQ-learning Agent Wins: {q_wins}\\nMCTS Agent Wins: {mcts_wins}\\nDraws: {draws}\")\n",
    "\n",
    "\n",
    "# Start training both agents\n",
    "start_time = time.time()\n",
    "q_agent = QLearningAgent()\n",
    "mcts_agent = MCTS(simulations=20)\n",
    "env = ConnectFour()\n",
    "\n",
    "# Train both agents simultaneously with random moves\n",
    "#train_both_agents(env, q_agent, mcts_agent, episodes=10000)\n",
    "train_both_agents(env, q_agent, mcts_agent, episodes=50000)\n",
    "# Test agents after training\n",
    "#test_agents(q_agent, mcts_agent, games=100)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b843c051-6922-476e-9e97-851f243469f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500/5000 - Epsilon: 0.7618\n",
      "Episode 1000/5000 - Epsilon: 0.5723\n",
      "Episode 1500/5000 - Epsilon: 0.4272\n",
      "Episode 2000/5000 - Epsilon: 0.3208\n",
      "Episode 2500/5000 - Epsilon: 0.2386\n",
      "Episode 3000/5000 - Epsilon: 0.1782\n",
      "Episode 3500/5000 - Epsilon: 0.1500\n",
      "Episode 4000/5000 - Epsilon: 0.1500\n",
      "Episode 4500/5000 - Epsilon: 0.1500\n",
      "Episode 5000/5000 - Epsilon: 0.1500\n",
      "--- 863.4163637161255 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Connect Four Environment\n",
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.rows = 6\n",
    "        self.cols = 7\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return tuple(map(tuple, self.board))\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def make_move(self, col):\n",
    "        if not self.is_valid_move(col):\n",
    "            return False, None\n",
    "        for row in range(self.rows - 1, -1, -1):\n",
    "            if self.board[row, col] == 0:\n",
    "                self.board[row, col] = self.current_player\n",
    "                reward, done = self.check_game_status()\n",
    "                self.current_player = 3 - self.current_player\n",
    "                return reward, done\n",
    "        return 0, False\n",
    "\n",
    "    def check_game_status(self):\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.cols - 3):\n",
    "                if self.check_sequence(row, col, 0, 1):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols):\n",
    "                if self.check_sequence(row, col, 1, 0):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols - 3):\n",
    "                if self.check_sequence(row, col, 1, 1) or self.check_sequence(row + 3, col, -1, 1):\n",
    "                    return (1, True) if self.current_player == 1 else (-1, True)\n",
    "        if not any(self.is_valid_move(c) for c in range(self.cols)):\n",
    "            return (0, True)  # Draw case\n",
    "        return (0, False)\n",
    "\n",
    "    def check_sequence(self, row, col, row_delta, col_delta):\n",
    "        piece = self.board[row, col]\n",
    "        if piece == 0:\n",
    "            return False\n",
    "        for i in range(1, 4):\n",
    "            if self.board[row + i * row_delta, col + i * col_delta] != piece:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "# Q learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.8, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.1):\n",
    "        self.q_table = defaultdict(lambda: np.zeros(7))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def choose_action(self, state, valid_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(valid_moves)\n",
    "        q_values = self.q_table[state]\n",
    "        return max(valid_moves, key=lambda col: q_values[col])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        q_values = self.q_table[state]\n",
    "        if done:\n",
    "            q_values[action] += self.alpha * (reward - q_values[action])\n",
    "        else:\n",
    "            next_q_values = self.q_table[next_state]\n",
    "            q_values[action] += self.alpha * (reward + self.gamma * np.max(next_q_values) - q_values[action])\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "\n",
    "# MCTS Node\n",
    "class Node:\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "        self.children = {}\n",
    "\n",
    "    def best_child(self):\n",
    "        return max(self.children.items(), key=lambda x: x[1].wins / (x[1].visits + 1e-6))[0]\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, simulations=100, q_agent=None):  \n",
    "        self.simulations = simulations\n",
    "        self.q_agent = q_agent  # Allow MCTS to use Q learning agent\n",
    "\n",
    "    def search(self, env, root):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.run_simulation, env, root) for _ in range(self.simulations)]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "        return root.best_child()\n",
    "\n",
    "    def run_simulation(self, env, root):\n",
    "        node = root\n",
    "        temp_env = ConnectFour()\n",
    "        temp_env.board = np.copy(env.board)\n",
    "        temp_env.current_player = env.current_player\n",
    "\n",
    "        # Selection\n",
    "        while node.children:\n",
    "            action = node.best_child()\n",
    "            temp_env.make_move(action)\n",
    "            node = node.children[action]\n",
    "\n",
    "        # Expansion\n",
    "        valid_moves = [c for c in range(env.cols) if temp_env.is_valid_move(c)]\n",
    "        for move in valid_moves:\n",
    "            temp_env.make_move(move)\n",
    "            node.children[move] = Node(temp_env.get_state())\n",
    "\n",
    "        # Simulation: More weight towards Q-learning moves\n",
    "        state = temp_env.get_state()\n",
    "        done = False\n",
    "        reward = 0  \n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if temp_env.is_valid_move(c)]\n",
    "            if not valid_moves:  # Handle case where no moves are left\n",
    "                break\n",
    "\n",
    "            if random.uniform(0, 1) < 0.7 and self.q_agent:  # 70% chance to use Q learning\n",
    "                action = self.q_agent.choose_action(state, valid_moves)\n",
    "            else:\n",
    "                action = random.choice(valid_moves)  # 30% random move\n",
    "            reward, done = temp_env.make_move(action) \n",
    "            state = temp_env.get_state()\n",
    "        \n",
    "        # Backpropagation\n",
    "        for child in node.children.values():\n",
    "            child.wins += reward\n",
    "            child.visits += 1\n",
    "\n",
    "\n",
    "# Simultaneous Training of Q-learning and MCTS with Random Moves\n",
    "def train_both_agents(env, q_agent, mcts_agent, episodes=50000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "            \n",
    "            # Alternate between Q-learning, MCTS, or Random Move\n",
    "            move_choice = random.uniform(0, 1)\n",
    "            if move_choice < 0.25:  # 25% chance for Q-learning\n",
    "                action = q_agent.choose_action(state, valid_moves)  # Q-learning move\n",
    "            elif move_choice < 0.5:  # 25% chance for MCTS\n",
    "                root = Node(state)\n",
    "                action = mcts_agent.search(env, root)  # MCTS move\n",
    "            else:  # 50% random move\n",
    "                action = random.choice(valid_moves)\n",
    "\n",
    "            reward, done = env.make_move(action)\n",
    "            next_state = env.get_state()\n",
    "\n",
    "            if move_choice < 0.25:  # Only update Q-learning if it chose the move\n",
    "                q_agent.update_q_table(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Print progress every 500 episodes\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes} - Epsilon: {q_agent.epsilon:.4f}\")\n",
    "\n",
    "\n",
    "# Testing the trained Q-learning agent vs. MCTS agent\n",
    "def test_agents(q_agent, mcts_agent, games=100):\n",
    "    env = ConnectFour()\n",
    "    q_wins, mcts_wins, draws = 0, 0, 0\n",
    "    \n",
    "    for game in range(games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "            \n",
    "            # Alternate between Q-learning and MCTS during the game\n",
    "            move_choice = random.uniform(0, 1)\n",
    "            if move_choice < 0.5:\n",
    "                action = q_agent.choose_action(state, valid_moves)\n",
    "            else:\n",
    "                root = Node(state)\n",
    "                action = mcts_agent.search(env, root)\n",
    "            \n",
    "            reward, done = env.make_move(action)\n",
    "            state = env.get_state()\n",
    "        \n",
    "        if reward == 1:\n",
    "            q_wins += 1\n",
    "        elif reward == -1:\n",
    "            mcts_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    print(f\"Results after {games} games:\\nQ-learning Agent Wins: {q_wins}\\nMCTS Agent Wins: {mcts_wins}\\nDraws: {draws}\")\n",
    "\n",
    "\n",
    "# Start training both agents\n",
    "#start_time = time.time()\n",
    "#q_agent = QLearningAgent(alpha=0.1, gamma=0.8, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.1)\n",
    "#mcts_agent = MCTS(simulations=100)  # Increased simulations\n",
    "#env = ConnectFour()\n",
    "\n",
    "#train_both_agents(env, q_agent, mcts_agent, episodes=50000)\n",
    "\n",
    "# Start training both agents with optimized parameters\n",
    "start_time = time.time()\n",
    "#q_agent = QLearningAgent(alpha=0.2, gamma=0.8, epsilon=1.0, epsilon_decay=0.9999, epsilon_min=0.2)\n",
    "#mcts_agent = MCTS(simulations=50)  # Reduced simulations for faster training\n",
    "q_agent = QLearningAgent(alpha=0.25, gamma=0.8, epsilon=1.0, epsilon_decay=0.9999, epsilon_min=0.15)\n",
    "mcts_agent = MCTS(simulations=75)  # More simulations to improve MCTS decisions\n",
    "\n",
    "env = ConnectFour()\n",
    "\n",
    "train_both_agents(env, q_agent, mcts_agent, episodes=5000)  # Reduced episodes for quicker results\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61a010-57c6-43fd-9840-e6e79d2335f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6eaebad-c0fd-4c78-a463-08c1920e09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are separate functions for testing:\n",
    "\n",
    "#MCTS vs. Q-Learning\n",
    "#MCTS vs. Random\n",
    "#Q-Learning vs. Random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c135eaf-a818-4b89-af4f-24d3733c9117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49391c-34c2-4453-882b-e7f58bb81c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2230b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTS vs. Q-Learning (100 games): MCTS Wins: 62, Q-Learning Wins: 33, Draws: 5\n",
      "MCTS vs. Random (100 games): MCTS Wins: 90, Random Wins: 10, Draws: 0\n",
      "Q-Learning vs. Random (100 games): Q-Learning Wins: 78, Random Wins: 22, Draws: 0\n"
     ]
    }
   ],
   "source": [
    "def test_mcts_vs_qlearning(mcts_agent, q_agent, games=100):\n",
    "    \"\"\" Test Monte Carlo Tree Search vs. Q-learning Agent \"\"\"\n",
    "    env = ConnectFour()\n",
    "    mcts_wins, q_wins, draws = 0, 0, 0\n",
    "    \n",
    "    for _ in range(games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "\n",
    "            if env.current_player == 1:  # MCTS move\n",
    "                root = Node(state)\n",
    "                action = mcts_agent.search(env, root)\n",
    "            else:  # Q learning move\n",
    "                action = q_agent.choose_action(state, valid_moves)\n",
    "\n",
    "            reward, done = env.make_move(action)\n",
    "            state = env.get_state()\n",
    "\n",
    "        if reward == 1:  # MCTS wins\n",
    "            mcts_wins += 1\n",
    "        elif reward == -1:  # Q learning wins\n",
    "            q_wins += 1\n",
    "        else:  # Draw\n",
    "            draws += 1\n",
    "\n",
    "    print(f\"MCTS vs. Q-Learning ({games} games): MCTS Wins: {mcts_wins}, Q-Learning Wins: {q_wins}, Draws: {draws}\")\n",
    "\n",
    "\n",
    "def test_mcts_vs_random(mcts_agent, games=100):\n",
    "    \"\"\" Test Monte Carlo Tree Search vs. Random Moves \"\"\"\n",
    "    env = ConnectFour()\n",
    "    mcts_wins, random_wins, draws = 0, 0, 0\n",
    "    \n",
    "    for _ in range(games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "\n",
    "            if env.current_player == 1:  # MCTS move\n",
    "                root = Node(state)\n",
    "                action = mcts_agent.search(env, root)\n",
    "            else:  # Random move\n",
    "                action = random.choice(valid_moves)\n",
    "\n",
    "            reward, done = env.make_move(action)\n",
    "            state = env.get_state()\n",
    "\n",
    "        if reward == 1:  # MCTS wins\n",
    "            mcts_wins += 1\n",
    "        elif reward == -1:  # Random wins\n",
    "            random_wins += 1\n",
    "        else:  # Draw\n",
    "            draws += 1\n",
    "\n",
    "    print(f\"MCTS vs. Random ({games} games): MCTS Wins: {mcts_wins}, Random Wins: {random_wins}, Draws: {draws}\")\n",
    "\n",
    "\n",
    "def test_qlearning_vs_random(q_agent, games=100):\n",
    "    \"\"\" Test Q-learning Agent vs. Random Moves \"\"\"\n",
    "    env = ConnectFour()\n",
    "    q_wins, random_wins, draws = 0, 0, 0\n",
    "    \n",
    "    for _ in range(games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = [c for c in range(env.cols) if env.is_valid_move(c)]\n",
    "\n",
    "            if env.current_player == 1:  # Q-learning move\n",
    "                action = q_agent.choose_action(state, valid_moves)\n",
    "            else:  # Random move\n",
    "                action = random.choice(valid_moves)\n",
    "\n",
    "            reward, done = env.make_move(action)\n",
    "            state = env.get_state()\n",
    "\n",
    "        if reward == 1:  # Q-learning wins\n",
    "            q_wins += 1\n",
    "        elif reward == -1:  # Random wins\n",
    "            random_wins += 1\n",
    "        else:  # Draw\n",
    "            draws += 1\n",
    "\n",
    "    print(f\"Q-Learning vs. Random ({games} games): Q-Learning Wins: {q_wins}, Random Wins: {random_wins}, Draws: {draws}\")\n",
    "\n",
    "# Initialize agents\n",
    "\n",
    "\n",
    "# Run tests\n",
    "test_mcts_vs_qlearning(mcts_agent, q_agent, games=100)\n",
    "test_mcts_vs_random(mcts_agent, games=100)\n",
    "test_qlearning_vs_random(q_agent, games=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "521873be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce2bae-e7c1-438f-ba45-d7de68102b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Board dimensions\n",
    "ROW_COUNT = 6\n",
    "COLUMN_COUNT = 7\n",
    "SQUARESIZE = 100\n",
    "width = COLUMN_COUNT * SQUARESIZE\n",
    "height = (ROW_COUNT + 1) * SQUARESIZE  # Extra row for input\n",
    "RADIUS = SQUARESIZE // 2 - 5\n",
    "\n",
    "# Colors\n",
    "BLUE = (0, 0, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "YELLOW = (255, 255, 0)\n",
    "\n",
    "size = (width, height)\n",
    "screen = pygame.display.set_mode(size)\n",
    "\n",
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((ROW_COUNT, COLUMN_COUNT), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def is_valid_move(self, col):\n",
    "        return self.board[0, col] == 0\n",
    "\n",
    "    def make_move(self, col):\n",
    "        for row in range(ROW_COUNT - 1, -1, -1):\n",
    "            if self.board[row, col] == 0:\n",
    "                self.board[row, col] = self.current_player\n",
    "                self.current_player = 3 - self.current_player\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        for r in range(ROW_COUNT):\n",
    "            for c in range(COLUMN_COUNT - 3):\n",
    "                if self.board[r, c] == self.board[r, c+1] == self.board[r, c+2] == self.board[r, c+3] != 0:\n",
    "                    return self.board[r, c]\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            for c in range(COLUMN_COUNT):\n",
    "                if self.board[r, c] == self.board[r+1, c] == self.board[r+2, c] == self.board[r+3, c] != 0:\n",
    "                    return self.board[r, c]\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            for c in range(COLUMN_COUNT - 3):\n",
    "                if self.board[r, c] == self.board[r+1, c+1] == self.board[r+2, c+2] == self.board[r+3, c+3] != 0:\n",
    "                    return self.board[r, c]\n",
    "                if self.board[r+3, c] == self.board[r+2, c+1] == self.board[r+1, c+2] == self.board[r, c+3] != 0:\n",
    "                    return self.board[r+3, c]\n",
    "        return 0\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        return [c for c in range(COLUMN_COUNT) if self.is_valid_move(c)]\n",
    "\n",
    "def draw_board(board):\n",
    "    screen.fill(BLACK)\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT):\n",
    "            pygame.draw.rect(screen, BLUE, (c*SQUARESIZE, (r+1)*SQUARESIZE, SQUARESIZE, SQUARESIZE))\n",
    "            color = BLACK\n",
    "            if board[r, c] == 1:\n",
    "                color = RED\n",
    "            elif board[r, c] == 2:\n",
    "                color = YELLOW\n",
    "            pygame.draw.circle(screen, color, (c*SQUARESIZE + SQUARESIZE//2, (r+1)*SQUARESIZE + SQUARESIZE//2), RADIUS)\n",
    "    pygame.display.update()\n",
    "\n",
    "def play_game(ai_type):\n",
    "    env = ConnectFour()\n",
    "    running = True\n",
    "    turn = 1  # Human starts\n",
    "\n",
    "    draw_board(env.board)\n",
    "    \n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "            if event.type == pygame.MOUSEMOTION:\n",
    "                pygame.draw.rect(screen, BLACK, (0, 0, width, SQUARESIZE))\n",
    "                posx = event.pos[0]\n",
    "                pygame.draw.circle(screen, RED if turn == 1 else YELLOW, (posx, SQUARESIZE//2), RADIUS)\n",
    "                pygame.display.update()\n",
    "\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if turn == 1:  # Human move\n",
    "                    col = event.pos[0] // SQUARESIZE\n",
    "                    if env.is_valid_move(col):\n",
    "                        env.make_move(col)\n",
    "                        draw_board(env.board)\n",
    "                        if env.check_winner():\n",
    "                            print(f\"Player {turn} wins!\")\n",
    "                            running = False\n",
    "                        turn = 2\n",
    "\n",
    "        if turn == 2 and running:  # AI move\n",
    "            pygame.time.delay(500)\n",
    "            valid_moves = env.get_valid_moves()\n",
    "            if ai_type == \"random\":\n",
    "                col = random.choice(valid_moves)\n",
    "            elif ai_type == \"mcts\":\n",
    "                col = random.choice(valid_moves)  # Placeholder for MCTS \n",
    "            elif ai_type == \"qlearning\":\n",
    "                col = random.choice(valid_moves)  # Placeholder for Q learning \n",
    "            env.make_move(col)\n",
    "            draw_board(env.board)\n",
    "            if env.check_winner():\n",
    "                print(f\"AI ({ai_type}) wins!\")\n",
    "                running = False\n",
    "            turn = 1\n",
    "\n",
    "        pygame.time.delay(100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ai_choice = input(\"Choose AI opponent: (random, mcts, qlearning): \").lower()\n",
    "    play_game(ai_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1ca29-ac7d-4d10-b36c-6c018d954a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9ae3-4e93-490b-961d-999695710150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0ae6f-ef95-41e6-a9cb-113b50c005ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b9f03-104c-4619-9ff1-125234b72b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d779e1-d379-4916-a93c-f8b9a7b645bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086317d9-3918-4804-990d-f1a6c744d3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14cebac-4905-417c-ab3f-82f3c4dd190d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535535f6-3f02-466e-8983-a7544f5b258b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0bcaca-8da3-4585-8871-d0575de0bc39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343228dd-aecd-4651-8df9-0c674646850f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f05297-80bf-450b-8078-708c141d1e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
